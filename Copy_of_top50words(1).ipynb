{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "from collections import Counter\n",
        "def generate_stop_list(num_files=10, top_n=50):\n",
        "    \"\"\"\n",
        "    Reads text from a sequence of files, cleans it, counts word frequencies,\n",
        "    and identifies the top N most frequent words (the stop-list). (didn't know if we should leave stop words like to , and ... out so i left them in .)\n",
        "    \"\"\"\n",
        "    all_text = \"\"\n",
        "    # 1. Read all text from the specified files article1-10.txt\n",
        "    print(\"\\n--- Step 1: Reading and Aggregating Text ---\")\n",
        "    files_found = 0\n",
        "    for i in range(1, num_files + 1):\n",
        "        filename = f'article{i}.txt'\n",
        "        try:\n",
        "            # Read the file content\n",
        "            with open(filename, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "                all_text += content + \"\\n\"\n",
        "                files_found += 1\n",
        "            print(f\"Successfully read: {filename}\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: File not found: {filename}. Skipping.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {filename}: {e}\")\n",
        "\n",
        "    if files_found == 0:\n",
        "        print(\"\\nERROR: Could not find any text files (article1.txt to article10.txt). Please ensure they are in the same directory.\")\n",
        "        return\n",
        "\n",
        "    # 2. Tokenize and Clean the Text\n",
        "    print(\"\\n--- Step 2: Cleaning and Tokenizing Text ---\")\n",
        "    # Convert to lowercase and remove all characters that are not letters or spaces.\n",
        "    cleaned_text = all_text.lower()\n",
        "    # Replace anything that is not a word character or whitespace with a space\n",
        "    cleaned_text = re.sub(r'[^a-z\\s]', ' ', cleaned_text)\n",
        "    # Tokenize: split by whitespace and filter out empty strings\n",
        "    words = [word for word in cleaned_text.split() if word]\n",
        "\n",
        "    print(f\"Total words collected and cleaned: {len(words):,}\")\n",
        "\n",
        "    # 3. Count Word Frequencies\n",
        "    word_counts = Counter(words)\n",
        "\n",
        "    # 4. Get the Top N Most Frequent Words\n",
        "    print(f\"\\n--- Step 3: Finding the Top {top_n} Most Frequent Words ---\")\n",
        "    most_common_words = word_counts.most_common(top_n)\n",
        "\n",
        "    # 5. Save the Stop-List to a File\n",
        "    output_filename = 'stop_list.txt'\n",
        "    try:\n",
        "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "            # Save words one per line\n",
        "            stop_list = [word for word, count in most_common_words]\n",
        "            f.write('\\n'.join(stop_list))\n",
        "        print(f\"\\nSUCCESS: The top {top_n} words have been saved to '{output_filename}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving file: {e}\")\n",
        "        return\n",
        "\n",
        "    # 6. Print the 50 Words\n",
        "    print(f\"\\n--- Top {top_n} Most Frequent Words (The Stop-List) ---\")\n",
        "    print(f\"| {'Word':<15} | {'Count':<7} |\")\n",
        "    print(\"-\" * 34)\n",
        "    for rank, (word, count) in enumerate(most_common_words, 1):\n",
        "        print(f\"| {word:<15} | {count:<7} |\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate_stop_list()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Step 1: Reading and Aggregating Text ---\n",
            "Successfully read: article1.txt\n",
            "Successfully read: article2.txt\n",
            "Successfully read: article3.txt\n",
            "Successfully read: article4.txt\n",
            "Successfully read: article5.txt\n",
            "Successfully read: article6.txt\n",
            "Successfully read: article7.txt\n",
            "Successfully read: article8.txt\n",
            "Successfully read: article9.txt\n",
            "Successfully read: article10.txt\n",
            "\n",
            "--- Step 2: Cleaning and Tokenizing Text ---\n",
            "Total words collected and cleaned: 3,860\n",
            "\n",
            "--- Step 3: Finding the Top 50 Most Frequent Words ---\n",
            "\n",
            "SUCCESS: The top 50 words have been saved to 'stop_list.txt'\n",
            "\n",
            "--- Top 50 Most Frequent Words (The Stop-List) ---\n",
            "| Word            | Count   |\n",
            "----------------------------------\n",
            "| the             | 247     |\n",
            "| and             | 165     |\n",
            "| of              | 130     |\n",
            "| to              | 82      |\n",
            "| in              | 75      |\n",
            "| a               | 74      |\n",
            "| by              | 46      |\n",
            "| data            | 44      |\n",
            "| is              | 43      |\n",
            "| as              | 36      |\n",
            "| football        | 32      |\n",
            "| or              | 32      |\n",
            "| s               | 31      |\n",
            "| that            | 28      |\n",
            "| for             | 28      |\n",
            "| with            | 23      |\n",
            "| games           | 22      |\n",
            "| was             | 20      |\n",
            "| video           | 20      |\n",
            "| it              | 19      |\n",
            "| an              | 18      |\n",
            "| game            | 18      |\n",
            "| internet        | 18      |\n",
            "| big             | 18      |\n",
            "| are             | 17      |\n",
            "| on              | 17      |\n",
            "| devices         | 16      |\n",
            "| such            | 15      |\n",
            "| from            | 14      |\n",
            "| computer        | 14      |\n",
            "| china           | 14      |\n",
            "| strike          | 14      |\n",
            "| first           | 13      |\n",
            "| computers       | 13      |\n",
            "| counter         | 13      |\n",
            "| world           | 12      |\n",
            "| be              | 12      |\n",
            "| which           | 12      |\n",
            "| has             | 12      |\n",
            "| country         | 12      |\n",
            "| f               | 11      |\n",
            "| aircraft        | 11      |\n",
            "| most            | 10      |\n",
            "| include         | 10      |\n",
            "| have            | 10      |\n",
            "| also            | 10      |\n",
            "| economics       | 10      |\n",
            "| air             | 10      |\n",
            "| ball            | 9       |\n",
            "| can             | 9       |\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cHm9B4k6ieO",
        "outputId": "10d73c24-783d-451b-8918-3f63ac6c0fe8"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}